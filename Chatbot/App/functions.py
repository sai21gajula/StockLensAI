# -*- coding: utf-8 -*-
"""test2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnGGawkmJzodKB4gvHmwFlriYd-KwF2t
"""

import os
import ast
import json
import time
import boto3
import operator
from typing import Literal
from typing import Annotated
from pinecone import Pinecone
from tavily import TavilyClient
from typing_extensions import TypedDict
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END
from langchain_aws import BedrockEmbeddings
from langchain_aws import ChatBedrockConverse
from pinecone.grpc import PineconeGRPC as Pinecone

os.environ['TAVILY_API_KEY'] = "API-KEY"
os.environ['AWS_ACCESS_KEY_ID'] = "API-KEY"
os.environ['AWS_SECRET_ACCESS_KEY'] = "API-KEY"
os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'
os.environ['Pinecone_Key'] = "API-KEY"
pc = Pinecone(api_key=os.environ['Pinecone_Key'])

bedrock_client = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-west-2'
)

bedrock_embeddings = BedrockEmbeddings(
    client=bedrock_client, region_name='us-west-2', model_id='amazon.titan-embed-text-v2:0'
)

bedrock_llama = ChatBedrockConverse(
    client=bedrock_client,
    region_name='us-west-2',
    model_id='us.meta.llama3-3-70b-instruct-v1:0',
    temperature=0.3,
    top_p=0.8,
    max_tokens=40
)

bedrock_llama_instruct = ChatBedrockConverse(
    client=bedrock_client,
    region_name='us-west-2',
    model_id='us.meta.llama3-2-11b-instruct-v1:0',
    temperature=0.3,
    top_p=0.8,
)
bedrock_llama_instruct_limit = ChatBedrockConverse(
    client=bedrock_client,
    region_name='us-west-2',
    model_id='us.meta.llama3-2-11b-instruct-v1:0',
    temperature=0.3,
    top_p=0.8,
    max_tokens=100
)

index = pc.Index('research')
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def generate_embeddings(body):
    start_time = time.time()
    model_id = model_id='amazon.titan-embed-text-v2:0'
    accept = "application/json"
    content_type = "application/json"
    response = bedrock_client.invoke_model(
        body=body, modelId=model_id, accept=accept, contentType=content_type
    )
    response_body = json.loads(response.get('body').read())
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: generate_embeddings",elapsed_time)
    return response_body

class MultiAgentState(TypedDict):
    question: str
    original_q:str
    CIK_value: str
    SEC_filing_tags: Annotated[list, operator.add]
    documents: str
    web_results: str
    links: Annotated[list, operator.add]
    answer: str
    halt_execution: bool
    halt_SEC: bool
    n3s_question: str

def rewrite_q(state: MultiAgentState) -> MultiAgentState:
    start_time = time.time()
    SYS_PROMPT = """
    You are a great rewriter. When given a user’s question related to the finance or stock market field, rewrite the question in more detail, ensuring it remains a single, cohesive question while covering the essence of the original query. If the user's question is not related to finance or the stock market, return -1.
    Field of Topic: "Finance/Stock Market"

    Input:
    User Input: [User's question]
    Output:

    If the user's question is related to finance or the stock market, rewrite the question in greater detail, making sure it can be used for both further processing by another LLM or web search.
    If the user's question is unrelated to finance or the stock market, return -1.
    Examples:

    User Input: "How can I invest in the stock market?"
    Output: "What are the steps to start investing in the stock market for beginners?"

    User Input: "What is the best time to buy stocks?"
    Output: "When is the optimal time to buy stocks for maximum returns?"

    User Input: "How do stock prices go up and down?"
    Output: "What factors cause fluctuations in stock prices in the stock market?"

    User Input: "What is a good strategy for stock trading?"
    Output: "What are the most effective stock trading strategies for long-term growth?"

    User Input: "What is a dividend in stock market?"
    Output: "What does a dividend mean in the context of stock market investing?"

    User Input: "What is the weather like in Paris?"
    Output: -1

    Be on point while generating a question!
    Return only re-written question, or -1. (Donot return anything else!)
"""



    question = state["question"]
    prompt = SYS_PROMPT.replace("[User's question]", question)
    llm = bedrock_llama
    rewritten_question = llm.invoke(prompt)


    rewritten_question_content = str(rewritten_question.content.strip())
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: rewrite_q",elapsed_time)

    if rewritten_question_content != str(-1):
        return {"question": rewritten_question_content, "halt_execution": False}
    else:
        return {"answer": "Please ask a valid question", "halt_execution": True}

def get_CIK(state: MultiAgentState) -> dict:
    start_time = time.time()
    SYS_PROMPT = """
Find the 10-digit CIK for the company mentioned in the given question. Retain leading zeros and return only the CIK value. If the CIK is not found, return -1.

Input:
User Input: [User's question]

Output:
Return only the CIK value or -1. Do not include any additional text.
"""
    question = state["question"]
    prompt = SYS_PROMPT.replace("[User's question]", question)
    llm = bedrock_llama
    rewritten_question = llm.invoke(prompt)

    rewritten_question_content = str(rewritten_question.content.strip())
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: get_CIK",elapsed_time)
    if rewritten_question_content != str(-1):
        return {"CIK_value": rewritten_question_content,"halt_SEC":False}
    else:
        return {"halt_SEC": True}

def get_SEC_filings_tags(state: MultiAgentState) -> MultiAgentState:
    start_time = time.time()
    question = state["question"]

    SYS_PROMPT = SYS_PROMPT = """
    You are an intelligent assistant that determines whether SEC (Securities and Exchange Commission) filings data is sufficient to answer a given question about a company’s financials, operations, or performance. Based on the user's question, decide if the SEC filings, reports, or disclosures can provide enough information to answer it. Below is the list of available SEC filing types and their descriptions. If the question is related to any of these forms and sufficient information can be obtained from them, respond with the keys of the most relevant forms in a list format.

    Here is the available SEC data:

    ['PX14A6G', '144', '10-Q', '8-K', '3', 'DEFA14A', 'DEF 14A', 'S-3ASR', '10-K', '5', 'SD', 'SC 13G/A', '25-NSE', '424B2', 'FWP', '4/A', 'S-8', 'S-8 POS', 'PX14A6N', 'CERT', '8-A12B', '3/A', '25', 'SC 13G', '8-K/A']

    Input:

    User Input: [User's question]

    Output:

    If the question can be answered sufficiently using SEC data, return all the key(s) of the most relevant form(s) in a list format.
    (Donot return anything else!)
"""

    prompt = SYS_PROMPT.replace("[User's question]", question)
    llm = bedrock_llama
    rewritten_question = llm.invoke(prompt)
    rewritten_question_content = ast.literal_eval(str(rewritten_question.content.strip()))
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: get_SEC_filings_tags",elapsed_time)

    return {"SEC_filing_tags":rewritten_question_content}

def retrieve_documents(state: MultiAgentState) -> dict:
    """Fetches relevant documents from DB based on user query."""
    start_time = time.time()
    query = state["question"]

    body = json.dumps({
        "inputText": query,
        "embeddingTypes": ["binary"]
    })

    vector = generate_embeddings(body)

    a = index.query(
    vector=vector["embeddingsByType"]["binary"],
    top_k=5,
    include_values=False,
    include_metadata=True
)
    combined_text = '\n'.join([a.matches[i]['metadata']['text'] for i in range(len(a.matches))])
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: retrieve_documents",elapsed_time)

    if combined_text:
        return {"documents": combined_text}
    else:
        return {"halt_SEC":True}

def web_search(state: MultiAgentState) -> dict:
    start_time = time.time()
    query = state["question"]
    search_results = tavily_client.search(query=query, num_results=3, search_depth="advanced")
    filtered_results = [doc for doc in search_results['results'] if (doc.get('score', 0)) > 0.2]
    result_lines = []
    links = []
    for doc in filtered_results:
        title = doc.get('title', 'No Title')
        url = doc.get('url', '')
        content = doc.get('content', 'No Title')
        result_lines.append(f"{title} - {content}")
        links.append(url)
    results = "\n".join(result_lines)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: web_search",elapsed_time)

    return {'web_results': results, 'links':links}

def final_gen_ans(state: MultiAgentState):
    start_time = time.time()
    if not state["halt_execution"]:
        question = state["question"]
        if state["halt_SEC"]:
            web_context = state["web_results"]

            SYS_PROMPT = """
        You will receive one of two types of input:

        Context generated from web search: This was created by processing data scraped from the web.
        Context generated from our personal database: This was created by retrieving relevant documents stored in our personal database.
        When answering, prioritize the Context generated from our personal database, ensuring it's fully detailed and accurate. If that answer lacks sufficient detail or is vague, supplement it with the Context generated from web search.


            question: {question}
            web_context: {web_context}
            personal_database_context: []

            Answer: [Answer based on the RAG-generated response, supplemented with the web context if needed.]
            Only show the final answer generated, nothing else! (Do not tell the source of data, just return answer), also keep the answer under 100 words!
                """
            prompt = SYS_PROMPT.replace("{question}", question).replace("{web_context}", web_context)
            llm = bedrock_llama_instruct
            answer_3 = llm.invoke(prompt)
            final_ans = answer_3.content.strip()

            end_time = time.time()
            elapsed_time = end_time - start_time
            print("For functions: final_gen_ans",elapsed_time)
            return {'answer':final_ans}
        else:
            web_context = state["web_results"]
            personal_database_context = state["documents"]

            SYS_PROMPT = """
        You will receive one of two types of input:

        Context generated from web search: This was created by processing data scraped from the web.
        Context generated from our personal database: This was created by retrieving relevant documents stored in our personal database.
        When answering, prioritize the Context generated from our personal database, ensuring it's fully detailed and accurate. If that answer lacks sufficient detail or is vague, supplement it with the Context generated from web search.


            question: {question}
            web_context: {web_context}
            personal_database_context: {personal_database_context}

            Answer: [Answer based on the RAG-generated response, supplemented with the web context if needed.]
            Only show the final answer generated, nothing else! (Do not tell the source of data, just return answer), also keep the answer under 100 words!
                """
            prompt = SYS_PROMPT.replace("{question}", question).replace("{web_context}", web_context).replace("{personal_database_context}", personal_database_context)
            llm = bedrock_llama_instruct
            answer_3 = llm.invoke(prompt)
            final_ans = answer_3.content.strip()

            end_time = time.time()
            elapsed_time = end_time - start_time
            print("For functions: final_gen_ans",elapsed_time)
            return {'answer':final_ans}
    else:
        return {'answer':"No answer found in prestored data."}

def suggested_Questions(state: MultiAgentState) -> dict:
    start_time = time.time()
    question = state["question"]
    answer = state.get("answer", "")


    SYS_PROMPT = """
You are a knowledgeable assistant tasked with generating relevant questions based on the given question and its answer with field. Based on the context provided, suggest three new questions that could logically follow the initial question. The new questions should explore related aspects or provide deeper insights into the topic.

question: {question}
answer: {answer}
field: Finance/ Stock market


Suggested Questions:
all Suggested Questions should be in new line. Only give questions, nothing else! Only make 3 new Questions!
    """



    prompt = SYS_PROMPT.replace("{question}", question).replace("{answer}", answer)
    llm = bedrock_llama_instruct_limit
    answer_3 = llm.invoke(prompt)
    final_ans = answer_3.content.strip()
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("For functions: suggested_Questions",elapsed_time)
    return {"n3s_question":str(final_ans)}

def route_query(state: MultiAgentState) -> Literal["get_CIK", "web_search", "final_gen_ans"]:
    """Routes query to both retrieval and web search, or to final answer if halt_execution is True."""
    if state["halt_execution"]:
        return "final_gen_ans"
    else:
        return "get_CIK", "web_search"

workflow = StateGraph(MultiAgentState)
workflow.add_node("rewrite_q", rewrite_q)
workflow.add_node("get_CIK", get_CIK)
workflow.add_node("get_SEC_filings_tags", get_SEC_filings_tags)
workflow.add_node("retrieve_documents", retrieve_documents)
workflow.add_node("web_search", web_search)
workflow.add_node("final_gen_ans", final_gen_ans)
workflow.add_node("Follow_up_Questions", suggested_Questions)
intermediates = ["get_CIK", "web_search", "final_gen_ans"]
workflow.set_entry_point("rewrite_q")
workflow.add_conditional_edges("rewrite_q",route_query,intermediates)
workflow.add_edge("get_CIK", "get_SEC_filings_tags")
workflow.add_edge("get_SEC_filings_tags", "retrieve_documents")
workflow.add_edge(["retrieve_documents","web_search"], "final_gen_ans")
workflow.add_edge("final_gen_ans", "Follow_up_Questions")
workflow.add_edge("Follow_up_Questions", END)
graph = workflow.compile()